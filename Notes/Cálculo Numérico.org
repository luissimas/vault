#+title:Cálculo Numérico

Cálculo numérico (ou análise numérica), é o ramo do cálculo que estuda algoritmos de aproximação numérica para diversos problemas da matemática.

* Erros e sistemas de ponto flutuante
Em se tratando de métodos de aproximação, a existência de erros é inevitável. Portanto, são necessários métodos para entender o comportamento desses erros e estabelecer resultados confiáveis para os mesmos.

Se em uma dada medida o valor esperado é $x$ e o valor efetivamente medido (ou aproximado) é $x_0$, então o *erro absoluto* é dado por

$$ |x-x_0| $$

e o *erro relativo* é

$$ \frac{|x-x_0|}{x} $$

Note que essa definição de erro depende do valor esperado (ou o valor real, não aproximado). O problema é que muitas vezes esse valor não é conhecido e não pode ser obtido facilmente. Entretanto, é possível obter um *limitante superior* para os erros que garante que o erro não excederá um determinado valor.

Se sabemos que $x_m < x < x_M$, então a aproximação for dada por $\frac{x_M + x_m}{2}$, o *erro absoluto* está limitado superiormente por

$$ \frac{x_M-x_m}{2} $$

e como $x_m < x$, o *erro relativo* está limitado superiormente por

$$ \frac{x_M - x_m}{2x_m} $$

** Erros de representação
Muitas vezes os erros podem surgir de sua representação. Em geral, a representação de um número real $x$ em uma base $b$ se dá através de uma soma (possivelmente infinita) de potências de $b$:

$$
x=s \sum_{k = - \infty}^{N} x_k b^k
$$

onde $b, x_k \in \mathbb{N}, N \in \mathbb{Z}, b>1, x_N > 0, 0 \leq x_k < b$ e $s$ é o sinal de $x$.   Nessas condições, garante-se que todo número real tem uma *única representação* na base $b$.

*** Dígitos significativos corretos
Dado um número $x$ com sua representação representada por $(x_Nx_{N-1}\dots x_1x_0,x_{-1}x_{-2}\dots})_b$, se o erro absoluto é menor que $\frac{1}{2}b^m$, dizemos que $x_m$ é um dígito significativo exato.

*** Sistemas de Ponto Flutuante
Os /sistemas de ponto flutuante (SPF)/ consistem em uma maneira padronizada de representar números em uma base. Um SPF assume que o número de dígitos a serem utilizados é fixo e que os expoentes permitidos estão em um intervalo bem definido.

Um SPF é composto de $4$ números inteiros $(b, d, e_m, e_M)$, onde $b$ é a base, $d$ é o número de dígitos (depois da vírgula) da representação, e os expoentes devem estar entre os limites inferior e superior $e_m$ e $e_M$.

Note que se o número tiver mais dígitos que $d$, apenas os $d$ mais significativos são representados. Veja também que caso o número só possa ser representado na forma normal com um expoente fora do intervalo $[e_m, e_M]$ ele não pode ser representado no SPF. Dessa forma, *nem todo número real* pode ser *representado* exatamente em um dado SPF, e é possível que *números diferentes* tenham a *mesma representação*.

Um número qualquer, diferente de $0$ representado em um dado SPF $(b, d, e_m, e_M)$ tem a forma:

$$s 0, d_1d_2d_3\dots d_d \times b^e$$

onde $s$ é o sinal, $d_1 \dots d_d$ é a /mantissa/, com $d_1 \neq 0$ e $e_m\leq e \leq e_M$.

Os sistemas de ponto flutuante possuem problemas inerentes de precisão que se tornam mais claros ainda na aplicação de operações sobre os números. É possível que o produto de dois números não exista no SPF, que as propriedades básicas das operações entre números reais não sejam satisfeitas, entre outros problemas que resultam do uso de apenas um subconjunto dos reais para representação numérica.

* Resolução de sistemas lineares
Um sistema de equações lineares é um conjunto de $n$ equações em $k$ variáveis, geralmente escrito na seguinte forma:

\begin{cases}
    a_{11}x_1 + \cdots + a_{1k}x_k = b_1 \\
    a_{21}x_1 + \cdots + a_{2k}x_k = b_2 \\
    \quad \vdots \quad + \ddots + \quad \vdots \quad = \space \vdots \\
    a_{n1}x_1 + \cdots + a_{nk}x_k = b_n \\
\end{cases}

ou na forma matricial

\begin{gather}

  \begin{bmatrix}
  a_{11} & \cdots & a_{1k} \\
  a_{21} & \cdots & a_{2k} \\
  \vdots & \ddots & \vdots \\
  a_{n1} & \cdots & a_{nk}
  \end{bmatrix}
  \cdot
  \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_k\end{bmatrix}
  =
  \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_k\end{bmatrix}
  \text{, ou} \quad Ax = b

\end{gather}

Uma solução para o sistema linear é um *vetor* de números $(x_1, x_2, \dots, x_n)$ tais que a equação acima seja satisfeita. Existem diversos métodos para encontrar a solução de sistemas lineares, porém algumas delas podem rapidamente se tornar *muito custosas computacionalmente* quando aplicadas a sistemas lineares com um grande número de equações.

A ideia dos *métodos numéricos* para a resolução de sistemas de equações lineares é encontrar *métodos eficientes* para a resolução desses sistemas. Esses métodos se dividem em duas grandes categorias: os /métodos diretos/ e os /métodos iterativos/.

** Métodos diretos
Os métodos diretos para a resolução de sistemas lineares oferecem a *solução exata* (exceto quando há erros de arredondamento introduzidos pela máquina) com um *número finito de operações*. Entretanto, esses métodos podem se tornar inviáveis dependendo da estrutura ou dimensão do sistema.

Esses métodos se baseiam na seguinte propriedade de manipulação de sistemas lineares:

A solução de um sistema de equações lineares $Ax = b$ *não se altera* se aplicarmos as seguintes operações nas linhas de $A$:

1. Multiplicar uma equação por uma constante não nula;
2. Somar uma equação a um múltiplo de outra;
3. Trocar a ordem das equações

Essas operações podem ser aplicadas para transformar um sistema original $Ax = b$ em um *sistema equivalente* que possua a mesma solução, mas tenha uma resolução mais fácil.

*** Método da eliminação de Gauss
Esse método consiste em aplicar transformações no sistema até que se atinja uma *forma triangular*, ou seja

\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1k} \\
0 & a_{22} & \cdots & a_{2k} \\
\vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_{nk}
\end{bmatrix}

A partir do momento em que se chega ao sistema triangular, a resolução consiste apenas em aplicar substituição reversa nesse sistema.

O número total de operações $k$ para esse método é dado pela seguinte equação:

$$
k = \frac{2}{3}n^3 + \frac{3}{2}n ^2 - \frac{7}{6}n
$$

em que $n$ é o número de equações.

Uma preocupação com esse método é a propagação dos erros de arredondamento da máquina nas operações entre as linhas. Uma maneira de reduzir esse problema é usar o chamado *pivoteamento*, que consiste em, através da troca de linhas, fazer com que o pivô (elemento da diagonal que será usado para na eliminação) tenha o maior valor em módulo possível dentre os demais elementos da coluna.

*** Fatoração LU
O método da fatoração (ou decomposição) LU consiste em, dado um sistema linear na forma $Ax = b$, *decompor a matriz dos coeficientes* $A$ em um *produto de duas matrizes* $L$ e $U$, em que $L$ (/Lower/) é uma *matriz triangular inferior* com diagonal unitária e $U$ (/Upper/) é uma *matriz triangular superior*.

O custo computacional desse método é idêntico ao do método da eliminação de Gauss, porém com o método da fatoração é possível *reutilizar as operações de escalonamento* feitas na matriz dos coeficientes para *resolver outros sistemas* com a *mesma matriz de coeficientes* e *matrizes* $b$ *diferentes*.

Tomando $A = LU$, o sistema $Ax = b$ pode ser reescrito na forma $LUx = b$, que pode ser reescrito no seguinte sistema:

\begin{cases}
  Ly = b\\
  Ux = y
\end{cases}

Note que cada equação do sistema é também um *sistema*, dessa forma pode-se resolver o primeiro sistema para encontrar a solução $y$, e depois o segundo sistema, obtendo a solução $x$ do sistema original.

A matriz $U$ é a *matriz resultante* do processo de *eliminação de Gauss*, enquanto a matriz $L$ é composta pelos *fatores de multiplicação* usados para zerar cada elemento no processo.

Note que o fato da matriz $L$ ser composta pelas operações inversas utilizadas no escalonamento garante que a operação é reversível, portanto os elementos da matriz $B$ não se alteram de posição mesmo que as linhas de $A$ sejam alteradas em $U$ pelo processo de escalonamento.

** Métodos iterativos
Os métodos iterativos consistem na aplicação de sucessivas operações em um dado valor inicial para se obter uma *solução aproximada* para um sistema linear.

Em alguns casos esse tipo de método não é capaz de encontrar uma solução, isso ocorre quando o método não converge para o valor da solução. Sendo assim, a aplicação desse tipo de método deve sempre estar acompanhada de uma *análise de convergência* para o dado problema. Apesar disso, em algumas situações os métodos iterativos são preferíveis aos métodos diretos, principalmente quando o sistema linear é de *grande porte*, sistemas para os quais uma solução por um método direto representa um grande custo computacional.

*** Método do ponto fixo
O método do ponto fixo se baseia na ideia de que há pontos (chamados de pontos fixos) de funções que, quando aplicados à função resultam neles mesmos.

A partir de uma equação da forma $g(x) = 0$, é possível reescrevê-la na forma $x = f(x)$. Dessa forma, é possível criar uma *sequência recursiva* na forma:

$$x_{n} = f(x_{n-1})$$

Sendo que cada $x_n$ é uma nova *aproximação do ponto fixo*. Dessa forma, se a função $f$ é uma função para a qual o método do ponto fixo converge, então a sequência $(x_n)$ *converge para o ponto fixo* da função, ou seja, o ponto $x$ tal que $f(x) = 0$.

Note que é necessário assumir um valor inicial para $x_0$, esse valor é uma aproximação inicial, que será refinado através das sucessivas iterações de aplicação do método.

Veja que é necessário verificar se a função $f$ é uma função para a qual o método converge, isso pode ser verificado através do seguinte critério:

Seja $x=f(x)$ uma equação escrita na forma de ponto fixo, com $f(x)$ e sua derivada contínua no intervalo $[a, b]$, o qual contém uma ou mais raízes desta equação. Se $|f'(x)| < 1$ para todo $x \in (a,b)$, então a sequência $x_n = f(x_{n-1})$ converge para uma raiz $\epsilon \in (a,b)$ da equação original.

Definido o método do ponto fixo, vamos agora ver como é possível aplicá-lo à resolução de sistemas lineares. Consideramos um sistema de equações lineares de solução única, com $n$ equações e variáveis:

\begin{cases}
    a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1 \\
    a_{21}x_1 + a_{22}x_2 +\cdots + a_{2n}x_n = b_2 \\
    \quad \vdots \quad + \quad \vdots \quad + \ddots + \quad \vdots \quad = \quad \vdots \\
    a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n = b_n \\
\end{cases}

Na forma matricial:

\begin{gather}

AX = B \text{, onde}
A =
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1k} \\
a_{21} & a_{22} & \cdots & a_{2k} \\
\vdots & \vdots &\ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nk}
\end{bmatrix}
\quad
X =
\begin{bmatrix}
x_1\\
x_2\\
\vdots\\
x_n\\
\end{bmatrix}
B =
\begin{bmatrix}
b_1\\
b_2\\
\vdots\\
b_n\\
\end{bmatrix}

\end{gather}

A convergência dos métodos iterativos depende do sistema linear considerado. Existem algumas *condições de convergência* que podem ser expressas em termos de normas vetoriais ou matriciais. Para os dois métodos apresentados a seguir, uma *condição suficiente para a convergência* é que a matriz $A$ seja *diagonalmente dominante*, isto é, se para cada linha da matriz o valor absoluto do elemento diagonal da linha é maior que a soma dos valores absolutos dos outros elementos da linha.

**** Método de Jacobi
O método de Jacobi consiste em reescrever a equação matricial $AX = B$ em uma forma de ponto fixo $X = F(X)$. Para isso pode-se isolar os termos diagonais da matriz no sistema, chegando a um método iterativo cuja equação de recorrência é:

$$X^{(n)} = P + QX^{x-1}$$

onde $P = D^{-1}B$, $Q= I - D^{-1}A$ e $D$ é a matriz diagonal:

\begin{gather}

D=
\begin{bmatrix}
a_{11} & 0 & \cdots & 0 \\
0 & a_{22} & \cdots & 0 \\
\vdots & \vdots &\ddots & \vdots \\
0 & 0 & \cdots & a_{nk}
\end{bmatrix}

\end{gather}

Dessa forma, basta calcular os termos $P$ e $Q$ uma única vez e depois disso aplicar os valores de $X^{(n-1)}$ na equação de recorrência para obter os valores de $X^{(n)}$. Veja que é necessário iniciar com um valor para $X^{(0)}$.

**** Método de Gauss-Seidel
O método de Gauss-Seidel é muito semelhante ao método de Jacobi. A diferença entre os métodos consiste no fato de que, enquanto no método de Jacobi todos os elementos de $X^{(n)}$ são computados a partir de $X^{(n-1)}$, no método de Gauss-Seidel os elementos de $X^{(n)}$ já computados são usados para aproximar os elementos restantes de $X^{(n)}$.

Decompondo a matriz dos coeficientes do sistema linear $A$ numa soma da forma $A = D + L + U$ tal que:

\begin{gather}

D=
\begin{bmatrix}
a_{11} & 0 & \cdots & 0 \\
0 & a_{22} & \cdots & 0 \\
\vdots & \vdots &\ddots & \vdots \\
0 & 0 & \cdots & a_{nk}
\end{bmatrix}
\quad
L=
\begin{bmatrix}
0 & 0 & \cdots & 0 \\
a_{21} & 0 & \cdots & 0 \\
\vdots & \vdots &\ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & 0
\end{bmatrix}
\quad
U=
\begin{bmatrix}
0 & a_{12} & \cdots & a_{1n} \\
0 & 0 & \cdots & a_{2n} \\
\vdots & \vdots &\ddots & \vdots \\
0 & 0 & \cdots & 0
\end{bmatrix}

\end{gather}

Com essa decomposição, o sistema original $AX = B$ pode ser rescrito na forma $(D+L)X = B - UX$, que pode ser convertida na seguinte equação de recorrência:

$$X^{(n)} = P - QX^{(n-1)}$$

onde $P=(D+L)^{-1}B$ e $Q=(D+L)^{-1}U$.

Dessa forma, os termos $P$ e $Q$ também podem ser calculados uma única vez e após isso pode-se aplicar os valores de $X^{(n-1)}$ na equação de recorrência para obter os valores de $X^{(n)}$.

* Resolução de equações não lineares
A solução de equações não-lineares na forma $f(x) = 0$ nem sempre é simples. Quando tratamos de funções mais complexas, como polinômios de graus elevados, é praticamente impossível obter uma solução usando métodos convencionais. Por conta disso foram desenvolvidos métodos numéricos para resolver equações nessa forma.

** Método da Bissecção
O método da bissecção supõe apenas que a função $f$ seja contínua no intervalo $[a,b]$, sendo um dos métodos mais simples para determinar se existem raízes da função em um intervalo.

O seguinte resultado proporciona a base para esse método:

Seja $f: [a, b] \to \mathbb{R}$ uma função *contínua* tal que $f(a)f(b) < 0$, ou seja, $f$ *muda de sinal* no intervalo $[a,b]$. Então existe pelo menos um ponto $\bar{x} \in [a,b]$ tal que $f(\bar{x}) = 0$. Além disso, se $f'$ não muda de sinal em $(a,b)$, então $\bar{x}$ é a única raiz de $f$ nesse intervalo.

Com base no resultado acima, desenvolveu-se o seguinte método para encontrar uma aproximação para a raiz de $f$:

Consideramos o intervalo $I_0 = [a_0, b_0] = [a, b]$, onde $f$ muda de sinal, como intervalo inicial. Seja $x_M$ o *ponto médio* de $I_0$, ou seja, $x_M = \frac{(a_0 + b_0)}{2}$. Como $f(a)f(b) < 0$, sabe-se que $f(x_M)$ terá o mesmo sinal que $f(a)$ ou $f(b)$. Assim, escolhemos o *novo intervalo* $I_1 = [a_1, b_1]$ como sendo:

- Se $f(x_M) = 0$ (ou menor que uma precisão dada), temos que $x_M$ é *raiz* de $f$;
- $I_1 = [a, x_M]$ se $f(a)f(x_M) < 0$;
- $I_1 = [x_M, b]$ se $f(x_M)f(b) < 0$;

Dessa forma, a cada iteração obtemos um novo intervalo $I_i$ que *contém a raiz com metade do comprimento do intervalo anterior*. Note que, embora simples, o método da bissecção tem uma velocidade de convergência lenta. Após $n$ iterações, a aproximação obtida para a raiz da equação tem um *erro absoluto* dado por

$$|x-x^*| \leq \frac{b-a}{2^{n+1}}$$

** Método do Ponto Fixo
Um *ponto fixo* de uma dada função $\varphi$ é o número $p$ que quando aplicado na função resulta nele mesmo, ou seja, $\varphi(p) = p$.

A ideia do método do ponto fixo é *associar* o problema de se determinar os *pontos fixos* ao problema de se encontrar as *raízes* de uma função. Para encontrar as raízes de $f$, podemos escrever $f(x) = x - \varphi(x)$ e encontrar um ponto fixo de $\varphi$. Nesse caso, o ponto fixo de $\varphi$ será também a raiz de $f$.

Note que é necessário definir algum *método para a escolha da função* $\varphi$. Em geral, os seguintes resultados são levados em conta:

- Se $\varphi$ é contínua no intervalo $I = [a,b]$ e $\varphi(x) \in I$ para todo $x \in I$, então $\varphi$ *terá um ponto fixo* em $I$;
- Se $\varphi '$ existir em $(a,b)$ e existir uma constante $\alpha$ tal que  $|\varphi '(x)| \leq \alpha < 1$ para todo $x \in (a,b)$, então o ponto fixo será *único*.

Em geral, basta escolher uma função $\varphi$ que *satisfaça os dois resultados* acima.

Note que resolução da equação $x = \varphi(x)$ é um processo iterativo, definindo uma sequência $(p_n)$ tal que $p_{n+1} = \varphi(p_n)$. Dessa forma, se $p_n$ *convergir* para um valor $p$, então essa será a solução para $x = \varphi(x)$.

** Método de Newton-Raphson
O método de Newton-Raphson é resultado da aplicação do /Teorema de Taylor/ com o método do ponto fixo, resultando em um método que *converge mais rápido* para a solução, porém com *mais condições de convergência*.

Para este método, escreve-se a função $\varphi$ da seguinte maneira:

$$\varphi(x) = x - \frac{f(x)}{f'(x)}$$

Sendo assim, é gerada uma sequência $(p_n)$ na forma:

$$p_{n+1} = p_n - \frac{f(p_n)}{f'(p_n)}$$

Assim como no método do ponto fixo, a sequência *converge* para o valor da solução $x = \varphi(x)$.

Note que é assumido que $f'(x) \neq 0$ para todos os componentes da sequência gerada. Sendo assim, a função $f$ tem que ter uma derivada contínua e esta derivada não pode se anular no intervalo de busca da solução. Além disso, por conta do Teorema de Taylor, o ponto inicial $p_0$ deve estar próximo da solução para que a convergência seja garantida.

#+begin_src python :results output
  import numpy as np
  import math


  def f(x):
      return x * np.sin(x)


  def df(x):
      return np.sin(x) + x * np.cos(x)


  p0 = 0.9

  list = [p0]

  i = 1
  max = 8

  while i <= max:
      pn = list[i - 1]
      pnext = pn - (f(pn) / df(pn))

      list.append(pnext)
      print("Iteração %d: %.6f" % (i, pnext))
      # print("|p%d - p%d| = %.5f" % (i, i - 1, math.fabs(pnext - pn)))

      if list[i] == list[i - 1]:
          break

      i = i + 1
#+end_src

#+RESULTS:
: Iteração 1: 0.374973
: Iteração 2: 0.182943
: Iteração 3: 0.090957
: Iteração 4: 0.045416
: Iteração 5: 0.022700
: Iteração 6: 0.011349
: Iteração 7: 0.005674
: Iteração 8: 0.002837


*** Método da Secante
O método da secante segue a mesma ideia do método de Newton, porém *ao invés de usar a derivada* da função cuja raiz é buscada, *usa-se uma aproximação* para ela.

Com a definição de derivada, é possível aproximar $f'(p_n)$ e gerar a seguinte sequência:

$$p_{n+1} = p_n - \frac{f(p_n)(p_n - p_{n-1})}{f(p_n) - f(p_{n-1})}$$

#+begin_src python :results output
  import numpy as np


  def f(x):
      return np.sin(7 * x) - 0.2 * x


  p0 = 0.4
  p1 = 0.5

  list = [p0, p1]

  i = 1

  while list[i] != list[i - 1]:
      pn = list[i]
      pn1 = list[i - 1]

      pnext = pn - ((f(pn) * (pn - pn1)) / (f(pn) - f(pn1)))

      list.append(pnext)
      print("Iteração %d: %.6f" % (i, pnext))
      i = i + 1
#+end_src

#+RESULTS:
: Iteração 1: 0.436129
: Iteração 2: 0.436319
: Iteração 3: 0.436317
: Iteração 4: 0.436317
: Iteração 5: 0.436317
: Iteração 6: 0.436317
: Iteração 7: 0.436317

*** Método da Falsa Posição
Com uma modificação no método da Secante é possível obter um método para encontrar raízes de funções não-lineares. Se considerarmos um intervalo $I_0 = [p_0, p1]$ e o cálculo de $p_2$ como no método da secante, ao invés de descartar o elemento $p_0$ e fazer um novo cálculo agora baseado no intervalo de extremos $p_1$ e $p_2$, analisamos o sinal da função nos três pontos:

- Se $f(p_0)f(p_2) < 0$ e $f(p_1)f(p_2) > 0$, então existe uma raiz entre $p_0$ e $p_2$, e selecionamos estes valores para a próxima iteração.
- Se $f(p_1)f(p_2) < 0$ e $f(p_0)f(p_2) > 0$, então existe uma raiz entre $p_1$ e $p_2$, e selecionamos estes valores para a próxima iteração.
- Se ambos os produtos tiverem o mesmo sinal, então é possível escolher qualquer combinação de valores para a próxima iteração.

* Interpolação de dados
É comum coletar conjuntos de dados em aplicações práticas, dados esses que podem ser vistos como *pontos* desconexos. O objetivo da interpolação é, dado um conjunto de pontos, determinar uma *expressão capaz de gerar todos esses pontos*. A ideia é que, obtida uma expressão que gera todos os pontos de dados coletados, é possível fazer *inferência* sobre os dados com base no *comportamento do conjunto de dados*, que agora é *definido por uma função*.

Dado um conjunto de *pontos distintos* $P = \{x_0, x_1,\dots, x_n\}$ chamados /nós da interpolação/ e uma função $f(x)$ cuja *definição é desconhecida*, mas todos os *valores* $f(x_0), f(x_1), \dots, f(x_n)$ *são conhecidos* para todo $x_1$, o problema de interpolação de $f(x)$ consiste em obter uma função $g(x)$ tal que:

\begin{cases}
  g(x_0) &= f(x_0)\\
  g(x_1) &= f(x_1)\\
  & \vdots\\
  g(x_n) &= f(x_n)
\end{cases}

Ou seja, os valores da função $g$ correspondem aos valores da função $f$ *em todos os pontos* do conjunto $P$. Note que não há garantias de que as funções coincidem em todos os valores de seus intervalos, mas sim que elas coincidem nos pontos do conjunto $P$.

Muitas vezes, é conveniente tomar uma *função polinomial* como função interpoladora, ou seja, como a função que deseja-se ajustar para gerar os pontos do conjunto. Essa estratégia de interpolação é chamada de *interpolação polinomial*, e consiste em encontrar um polinômio de grau menor ou igual a $n$ que interpola a função $f$.

** Matriz de Vandermonde
Tomando uma função polinomial $g(x_k) = p_n(x_k)$ tal que $p_n(x_k) = a_0 + a_1x_k^1 + \dots + a_nx_k^n$, é possível obter o seguinte *sistema linear*:

\begin{cases}
   a_0 + a_1x_0^1 + \cdots + a_nx_0^n = f(x_0)\\
   a_0 + a_1x_1^1 + \cdots + a_nx_1^n = f(x_1)\\
    \; \: \vdots \; + \quad \vdots \; \: + \ddots + \quad \vdots \quad = \quad \vdots \\
   a_0 + a_1x_n^1 + \cdots + a_nx_n^n = f(x_n)
\end{cases}

Encontrando os coeficientes $a_i$, encontra-se também a função $g$.

Na representação matricial desse sistema na forma $Ax = B$:

\begin{gather}

\begin{bmatrix}
   1 & x_0 & \cdots & x_0^n \\
   1 & x_1 & \cdots & x_1^n \\
    \vdots & \vdots & \ddots & \vdots \\
   1 & x_n & \cdots & x_n^n
\end{bmatrix}
\cdot
\begin{bmatrix}
  a_0\\
  a_1\\
  \vdots\\
  a_n\\
\end{bmatrix}
 =
\begin{bmatrix}
  f(x_0)\\
  f(x_1)\\
  \vdots\\
  f(x_n)\\
\end{bmatrix}

\end{gather}

A matriz $A$ é uma /matriz de Vandermonde/, que *possui solução única*, ou seja, existe um único polinômio $p_n(x)$ que gera os valores dados por $f(x_1)$.

Abaixo é possível verificar uma implementação simples do método de interpolação polinomial utilizando matriz de Vandermonde. O problema consiste em gerar as matrizes $A$ e $B$ do sistema linear com base nos valores de $x_i$ e $f(x_i)$ para então calcular os coeficientes $a_i$ do polinômio $p_n(x)$.

#+begin_src python :results output
  import numpy as np


  def vandermonde(x, fx):
      n = len(x)
      A = np.empty((n, n))
      B = np.empty((n))

      for i in range(0, n):
          A[i, 0] = 1

          for j in range(1, n):
              A[i, j] = A[i, j - 1] * x[i]

          B[i] = fx[i]

      return A, B


  # x = [0, 1, 2, 3]
  # fx = [1, 1, 2, 6]
  x = [-6, 6]
  fx = [9, 7]
  A, B = vandermonde(x, fx)
  x = np.linalg.solve(A, fx)

  print(x)
#+end_src

#+RESULTS:
: [ 8.         -0.16666667]

** Forma de Lagrange
A forma de Lagrange nos permite obter o polinômio que interpola a função $f$ sem passar pelo processo de resolução de um sistema linear. O método consiste em obter um *conjunto de polinômios* chamados /polinômios de Lagrange/, definidos por:

$$L_k(x) = \prod_{j \neq k}{\frac{x-x_j}{x_k - x_j}}$$

Esses polinômios têm a propriedade de valerem $1$ para $x_k$ e $0$ para todos os outros $x_l$. Obtidos os polinômios de Lagrange, o polinômio que interpola $f$ é definido por:

$$p_n(x) = \sum_{k=0}^n{f(x_k) L_k(x)}$$

Segue abaixo uma implementação da forma de Lagrange que retorna o valor de um $x$ qualquer dentro do intervalo da função $f$ dados os pontos e valores correspondentes de $f$.

#+begin_src python :results output
  import numpy as np


  def lagrange(x, xs, fx):
      n = len(xs) - 1 # Grau do polinômio
      sum = 0

      for k in range(0, n + 1):
          prod = 1

          for j in range(0, n + 1):
              if j != k:
                  prod = prod * (x - xs[j]) / (xs[k] - xs[j])

          sum = sum + (prod * fx[k])

      return sum


  x = 1.5
  xs = [-1, 0, 2]
  fx = [4, 1, -1]

  print(lagrange(x, xs, fx))
#+end_src

#+RESULTS:
: -1.0

* Aproximação de funções
Muitas vezes é possível obter pontos de dados através de resultados experimentais, o objetivo da aproximação de funções é relacionar duas ou mais variáveis através de uma função com base nos pontos de dados já conhecidos. Diversos métodos podem ser empregados para a aproximação de funções, alguns deles são descritos a seguir.

** Método dos Mínimos Quadrados
O Método dos Mínimos Quadrados consiste em, dado um *conjunto* de $n$ *pontos* $\{(x_0, f(x_0)), (x_1, f(x_1)), \dots, (x_{n-1}, f(x_{n-1}))\}$, *determinar uma função* $\varphi(x)$ que *melhor se aproxime* de $f$. Geralmente, escrevemos a função $\varphi$ como uma *combinação linear* de funções $g_i(x)$, tal que: $\varphi(x) = a_1g_1(x) + a_2g_2(x) + \dots + a_ig_i(x)$.

Primeiramente vamos definir o método para o caso básico no qual a função $\varphi$ é uma combinação linear de $2$ funções $g_1$ e $g_2$, após isso vamos generalizar o método para qualquer número de funções.

*** Duas variáveis
O problema de quadrados mínimos lineares discreto de duas variáveis consiste em, dadas duas funções $g_1(x)$ e $g_2(x)$, *determinar duas constantes* reais $a_1$ e $a_2$ tais que a função $\varphi(x) = a_1g_1(x) + a_2g_2(x)$ esteja o mais próximo possível dos pontos conhecidos. Naturalmente, para determinar o quão próximas são as aproximações é necessário definir o *erro*, que pode ser definido como $e(x_i) = \varphi(x_i) - f(x_i)$. Dessa forma, é possível transformar o problema em um problema de otimização da forma:

$$\min E(a_1, a_2) = \min \sum_{i=0}^{n-1}|e(x_i)|^2 = \min \sum_{i=0}^{n-1}|\varphi(x_i) - f(x_i)|^2$$

Resolvendo esse problema de otimização, obtém-se o seguinte sistema linear, que permite encontrar os valores das constantes $a_1$ e $a_2$ que minimizam o erro da estimativa para a função $f$:

\begin{cases}
  \displaystyle \left(\sum_{i=0}^{n-1}g_1(x_i)^2\right)a_1 + \left(\sum_{i=0}^{n-1}g_1(x_i)g_2(x_i)\right)a_2 = \sum_{i=0}^{n-1}g_1(x_i)f(x_i)\\
  \displaystyle \left(\sum_{i=0}^{n-1}g_1(x_i)g_2(x_i)\right)a_1 + \left(\sum_{i=0}^{n-1}g_2(x_i)^2\right)a_2 = \sum_{i=0}^{n-1}g_2(x_i)f(x_i)\\
\end{cases}

Esse método pode ser facilmente implementado da seguinte forma:

#+begin_src python :results output
  import numpy as np

  x = [-0.6975, -0.6762, 0.0176, 0.4512, 0.7559]
  fx = [2.0840, 2.0330, 0.8472, 0.6773, 0.4318]


  def g1(x):
      return 1
      # return np.sin(x)


  def g2(x):
      return np.exp(-x)
      # return np.cos(x)


  def a11(x):
      n = 0

      for i in range(len(x)):
          n = n + (g1(x[i]) ** 2)

      return n


  def a12(x):
      n = 0

      for i in range(len(x)):
          n = n + (g1(x[i]) * g2(x[i]))

      return n


  def a22(x):
      n = 0

      for i in range(len(x)):
          n = n + (g2(x[i]) ** 2)

      return n


  def y1(x, fx):
      n = 0

      for i in range(len(x)):
          n = n + (g1(x[i]) * fx[i])

      return n


  def y2(x, fx):
      n = 0

      for i in range(len(x)):
          n = n + (g2(x[i]) * fx[i])

      return n


  A = [[a11(x), a12(x)], [a12(x), a22(x)]]
  b = [y1(x, fx), y2(x, fx)]

  print("A = ", A)
  print("b = ", b)

  result = np.linalg.solve(A, b)

  print("a1 = ", result[0])
  print("a2 = ", result[1])
#+end_src

#+RESULTS:
: A =  [[5, 6.064121071226309], [6.064121071226309, 9.493189117566613]]
: b =  [6.0733, 9.650390822685234]
: a1 =  -0.08100654548009947
: a2 =  1.0683053077781681

*** Caso geral
No caso geral, o problema de quadrados mínimos consiste em *obter coeficientes* $a_1, a_2, \dots, a_n$ que minimizem a seguinte função:

$$\min E(a_1, a_2, \dots, a_n) = \min \sum_{i=1}^{m}|e(x_i)|^2 = \min \sum_{i=1}^{m}|\varphi(x_i) - f(x_i)|^2$$

sendo a função $\varphi(x)$ definida como:

$$\varphi(x_i) = a_1g_1(x_i) + a_2g_2(x_i) + \dots + a_ng_n(x_i)$$

Resolvendo esse problema de otimização através de derivadas parciais, obtém-se $n$ equações (as $n$ derivadas parciais igualadas a $0$). Manipulando as equações, é possível obter o seguinte sistema linear, que permite encontrar o valor dos coeficientes $a_1, a_2, \dots, a_n$:

\begin{cases}
  \displaystyle \left(\sum_{i=1}^{m}g_1(x_i)g_1(x_i)\right)a_1 + \left(\sum_{i=1}^{m}g_1(x_i)g_2(x_i)\right)a_2 + \dots + \left(\sum_{i=1}^{m}g_1(x_i)g_n(x_i)\right)a_n = \sum_{i=1}^{m}f(x_i)g_1(x_i)\\
  \displaystyle \left(\sum_{i=1}^{m}g_2(x_i)g_1(x_i)\right)a_1 + \left(\sum_{i=1}^{m}g_2(x_i)g_2(x_i)\right)a_2 + \dots + \left(\sum_{i=1}^{m}g_2(x_i)g_n(x_i)\right)a_n = \sum_{i=1}^{m}f(x_i)g_2(x_i)\\
  \qquad \vdots\\
  \displaystyle \left(\sum_{i=1}^{m}g_n(x_i)g_1(x_i)\right)a_1 + \left(\sum_{i=1}^{m}g_n(x_i)g_2(x_i)\right)a_2 + \dots + \left(\sum_{i=1}^{m}g_n(x_i)g_n(x_i)\right)a_n = \sum_{i=1}^{m}f(x_i)g_n(x_i)\\
\end{cases}

A implementação desse método consiste na construção e resolução deste sistema linear, como pode ser verificado a seguir:

#+begin_src python :results output
  import numpy as np
  import csv


  def termoA(x, g, i, j, m):
      num = 0

      for k in range(0, m):
          num = num + (g[i](x[k]) * g[j](x[k]))

      return num


  def termoB(x, fx, g, i, m):
      num = 0

      for k in range(0, m):
          num = num + (fx[k] * g[i](x[k]))

      return num


  def matrizAumentada(x, fx, g, n):
      m = len(x)
      A = np.empty((n, n))
      b = np.empty((n))

      for i in range(0, n):
          for j in range(i, n):
              A[i][j] = termoA(x, g, i, j, m)

              if i != j:
                  A[j][i] = A[i][j]

          b[i] = termoB(x, fx, g, i, m)

      return A, b


  x = [-1, -0.5, 0, 0.5, 1]
  fx = [8.4, 2.7, 2.6, 2.8, 5.6]
  g = [lambda x: 1, lambda x: np.sin(np.pi * x), lambda x: x * np.cos(np.pi * x)]
  n = len(g)


  A, b = matrizAumentada(x, fx, g, n)


  a = np.linalg.solve(A, b)

  print("A: ", A)
  print("b: ", b)
  print("Coeficientes: ", a)
#+end_src

#+RESULTS:
: A:  [[ 5.0000000e+00 -9.9579925e-17  0.0000000e+00]
:  [-9.9579925e-17  2.0000000e+00 -1.8369702e-16]
:  [ 0.0000000e+00 -1.8369702e-16  2.0000000e+00]]
: b:  [22.1  0.1  2.8]
: Coeficientes:  [4.42 0.05 1.4 ]

* COMMENT Integração numérica de funções
Integrais são ferramentas essenciais para diversas aplicações, entretanto na maioria das vezes não é possível obtê-las. Tendo isso em vista, foram desenvolvidos métodos numéricos para obter estimativas de *integrais definidas* de funções em dados intervalos.

A ideia que fundamenta os algoritmos de integração numérica é a de que é possível escrever uma integral definida em termos da soma de integrais de um *polinômio*. Esse resultado é chamado de *Fórmula de Newton-Cotes*, e fornece a base necessária para a aplicação de alguns dos métodos numéricos de integração.

Dada uma integral definida:

$$\int_{a}^{b}{f(x)}\, dx$$

obtendo-se a interpolação polinomial $p$ de $f$ através de *polinômios de Lagrange* tal que:

$$p_n(x) = \sum_{k=0}^n{f(x_k) L_k(x)}$$

sendo

$$L_k(x) = \prod_{j \neq k}{\frac{x-x_j}{x_k - x_j}}$$

a integral pode ser reescrita como:

$$\int_{a}^{b}{f(x)}\, dx = \sum_{k=0}^n{\left[f(x_k)\int_a^b{L_k(x)}\, dx\right]}$$

Dessa forma, o problema de integração se reduz ao problema de *integrar os polinômios de Lagrange*, cuja solução é muito mais simples.

A partir desses resultados desenvolveram-se dois métodos para a integração numérica de funções. A *diferença entre os métodos* reside no *grau do polinômio* obtido através da interpolação utilizado para aproximar a função.

** Método dos Trapézios
O método dos trapézios consiste em aproximar a função através de um polinômio de grau $1$ para então obter a área do gráfico da função aproximada.

#+attr_org: :width 200
[[file:~/dox/vault/Attachments/CN/metodotrapezios.png]]

Dada uma função $f(x)$, é possível aproximar sua integral definida em um intervalo $[a,b]$ através da seguinte definição:

$$\int_{x_0}^{x_1} \, dx \approx  h\frac{f(x_0) + f(x_1)}{2}$$

com $x_0 = a$ e $x_1 = b$.

Vale notar que esse método produz resultados *exatos* caso $f$ seja uma *função linear*. Caso contrário, o limitante superior para o erro da aproximação é dado pela seguinte expressão:

$$|E| \leq \frac{h^3}{12} \max(|f^{(2)}(x)|, \, x \in [a, b])$$

onde $h = b-a$.

O método pode ser implementado em python de uma maneira relativamente simples:

#+begin_src python :results output
  import numpy as np


  def trapezio(i, f):
      h = i[1] - i[0]

      return h * ((f(i[0]) + f(i[1])) / 2)


  i = [0, 2]
  f = lambda x: np.exp(x)

  print(trapezio(i, f))
#+end_src

#+RESULTS:
: 8.38905609893065

** Método de Simpson
O método de Simpsons consiste em aproximar a função por um polinômio de grau $2$ para então obter a área do gráfico da função aproximada.

#+attr_org: :width 200
[[file:~/dox/vault/Attachments/CN/metodosimpson.png]]

Dada uma função $f(x)$, é possível aproximar sua integral definida em um intervalo $[a,b]$ através da seguinte definição:

$$\int_{x_0}^{x_2} \, dx \approx \frac{h}{3}(f(x_0) + 4f(x_1) + f(x_2))$$

com $x_0 = a$, $x_1 = \frac{a+b}{2}$, $x_2 = b$.

Vale notar que esse método produz resultados *exatos* caso $f$ seja um *polinômio* de *grau* $3$. Caso contrário, o limitante superior para o erro da aproximação é dado pela seguinte expressão:

$$|E| \leq \frac{h^5}{90} \max(|f^{(4)}(x)| \, x \in [a, b])$$

onde $h = \frac{b-a}{2}$.

O método pode ser implementado em python de uma maneira relativamente simples:

#+begin_src python :results output
  import numpy as np


  def simpsons(i, f):
      x0 = i[0]
      x1 = (i[0] + i[1]) / 2
      x2 = i[1]

      h = (x2 - x0) / 2

      return h / 3 * (f(x0) + 4 * f(x1) + f(x2))


  i = [0, 1]
  f = lambda x: np.exp(-(x ** 2))

  print(simpsons(i, f))
#+end_src

#+RESULTS:
: 0.7471804289095104

* Resolução de sistemas de equações diferenciais
