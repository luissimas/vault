#+title:Cálculo Numérico

Cálculo numérico (ou análise numérica), é o ramo do cálculo que estuda algoritmos de aproximação numérica para diversos problemas da matemática.

* Erros e Sistemas de Ponto Flutuante
Em se tratando de métodos de aproximação, a existência de erros é inevitável. Portanto, são necessários métodos para entender o comportamento desses erros e estabelecer resultados confiáveis para os mesmos.

Se em uma dada medida o valor esperado é $x$ e o valor efetivamente medido (ou aproximado) é $x_0$, então o *erro absoluto* é dado por

$$ |x-x_0| $$

e o *erro relativo* é

$$ \frac{|x-x_0|}{x} $$

Note que essa definição de erro depende do valor esperado (ou o valor real, não aproximado). O problema é que muitas vezes esse valor não é conhecido e não pode ser obtido facilmente. Entretanto, é possível obter um *limitante superior* para os erros que garante que o erro não excederá um determinado valor.

Se sabemos que $x_m < x < x_M$, então a aproximação for dada por $\frac{x_M + x_m}{2}$, o *erro absoluto* está limitado superiormente por

$$ \frac{x_M-x_m}{2} $$

e como $x_m < x$, o *erro relativo* está limitado superiormente por

$$ \frac{x_M - x_m}{2x_m} $$

** Erros de representação
Muitas vezes os erros podem surgir de sua representação. Em geral, a representação de um número real $x$ em uma base $b$ se dá através de uma soma (possivelmente infinita) de potências de $b$:

$$
x=s \sum_{k = - \infty}^{N} x_k b^k
$$

onde $b, x_k \in \mathbb{N}, N \in \mathbb{Z}, b>1, x_N > 0, 0 \leq x_k < b$ e $s$ é o sinal de $x$.   Nessas condições, garante-se que todo número real tem uma *única representação* na base $b$.

*** Dígitos significativos corretos
Dado um número $x$ com sua representação representada por $(x_Nx_{N-1}\dots x_1x_0,x_{-1}x_{-2}\dots})_b$, se o erro absoluto é menor que $\frac{1}{2}b^m$, dizemos que $x_m$ é um dígito significativo exato.

*** Sistemas de Ponto Flutuante
Os /sistemas de ponto flutuante (SPF)/ consistem em uma maneira padronizada de representar números em uma base. Um SPF assume que o número de dígitos a serem utilizados é fixo e que os expoentes permitidos estão em um intervalo bem definido.

Um SPF é composto de $4$ números inteiros $(b, d, e_m, e_M)$, onde $b$ é a base, $d$ é o número de dígitos (depois da vírgula) da representação, e os expoentes devem estar entre os limites inferior e superior $e_m$ e $e_M$.

Note que se o número tiver mais dígitos que $d$, apenas os $d$ mais significativos são representados. Veja também que caso o número só possa ser representado na forma normal com um expoente fora do intervalo $[e_m, e_M]$ ele não pode ser representado no SPF. Dessa forma, *nem todo número real* pode ser *representado* exatamente em um dado SPF, e é possível que *números diferentes* tenham a *mesma representação*.

Um número qualquer, diferente de $0$ representado em um dado SPF $(b, d, e_m, e_M)$ tem a forma:

$$s 0, d_1d_2d_3\dots d_d \times b^e$$

onde $s$ é o sinal, $d_1 \dots d_d$ é a /mantissa/, com $d_1 \neq 0$ e $e_m\leq e \leq e_M$.

Os sistemas de ponto flutuante possuem problemas inerentes de precisão que se tornam mais claros ainda na aplicação de operações sobre os números. É possível que o produto de dois números não exista no SPF, que as propriedades básicas das operações entre números reais não sejam satisfeitas, entre outros problemas que resultam do uso de apenas um subconjunto dos reais para representação numérica.

* Resolução de sistemas lineares
Um sistema de equações lineares é um conjunto de $n$ equações em $k$ variáveis, geralmente escrito na seguinte forma:

\begin{cases}
    a_{11}x_1 + \cdots + a_{1k}x_k = b_1 \\
    a_{21}x_1 + \cdots + a_{2k}x_k = b_2 \\
    \quad \vdots \quad + \ddots + \quad \vdots \quad = \space \vdots \\
    a_{n1}x_1 + \cdots + a_{nk}x_k = b_n \\
\end{cases}

ou na forma matricial

\begin{gather}

  \begin{bmatrix}
  a_{11} & \cdots & a_{1k} \\
  a_{21} & \cdots & a_{2k} \\
  \vdots & \ddots & \vdots \\
  a_{n1} & \cdots & a_{nk}
  \end{bmatrix}
  \cdot
  \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_k\end{bmatrix}
  =
  \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_k\end{bmatrix}
  \text{, ou} \quad Ax = b

\end{gather}

Uma solução para o sistema linear é um *vetor* de números $(x_1, x_2, \dots, x_n)$ tais que a equação acima seja satisfeita. Existem diversos métodos para encontrar a solução de sistemas lineares, porém algumas delas podem rapidamente se tornar *muito custosas computacionalmente* quando aplicadas a sistemas lineares com um grande número de equações.

A ideia dos *métodos numéricos* para a resolução de sistemas de equações lineares é encontrar *métodos eficientes* para a resolução desses sistemas. Esses métodos se dividem em duas grandes categorias: os /métodos diretos/ e os /métodos iterativos/.

** Métodos diretos
Os métodos diretos para a resolução de sistemas lineares oferecem a *solução exata* (exceto quando há erros de arredondamento introduzidos pela máquina) com um *número finito de operações*. Entretanto, esses métodos podem se tornar inviáveis dependendo da estrutura ou dimensão do sistema.

Esses métodos se baseiam na seguinte propriedade de manipulação de sistemas lineares:

A solução de um sistema de equações lineares $Ax = b$ *não se altera* se aplicarmos as seguintes operações nas linhas de $A$:

1. Multiplicar uma equação por uma constante não nula;
2. Somar uma equação a um múltiplo de outra;
3. Trocar a ordem das equações

Essas operações podem ser aplicadas para transformar um sistema original $Ax = b$ em um *sistema equivalente* que possua a mesma solução, mas tenha uma resolução mais fácil.

*** Método da eliminação de Gauss
Esse método consiste em aplicar transformações no sistema até que se atinja uma *forma triangular*, ou seja

\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1k} \\
0 & a_{22} & \cdots & a_{2k} \\
\vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_{nk}
\end{bmatrix}

A partir do momento em que se chega ao sistema triangular, a resolução consiste apenas em aplicar substituição reversa nesse sistema.

O número total de operações $k$ para esse método é dado pela seguinte equação:

$$
k = \frac{2}{3}n^3 + \frac{3}{2}n ^2 - \frac{7}{6}n
$$

em que $n$ é o número de equações.

Uma preocupação com esse método é a propagação dos erros de arredondamento da máquina nas operações entre as linhas. Uma maneira de reduzir esse problema é usar o chamado *pivoteamento*, que consiste em, através da troca de linhas, fazer com que o pivô (elemento da diagonal que será usado para na eliminação) tenha o maior valor em módulo possível dentre os demais elementos da coluna.

*** Fatoração LU
O método da fatoração (ou decomposição) LU consiste em, dado um sistema linear na forma $Ax = b$, *decompor a matriz dos coeficientes* $A$ em um *produto de duas matrizes* $L$ e $U$, em que $L$ (/Lower/) é uma *matriz triangular inferior* com diagonal unitária e $U$ (/Upper/) é uma *matriz triangular superior*.

O custo computacional desse método é idêntico ao do método da eliminação de Gauss, porém com o método da fatoração é possível *reutilizar as operações de escalonamento* feitas na matriz dos coeficientes para *resolver outros sistemas* com a *mesma matriz de coeficientes* e *matrizes* $b$ *diferentes*.

Tomando $A = LU$, o sistema $Ax = b$ pode ser reescrito na forma $LUx = b$, que pode ser reescrito no seguinte sistema:

\begin{cases}
  Ly = b\\
  Ux = y
\end{cases}

Note que cada equação do sistema é também um *sistema*, dessa forma pode-se resolver o primeiro sistema para encontrar a solução $y$, e depois o segundo sistema, obtendo a solução $x$ do sistema original.

A matriz $U$ é a *matriz resultante* do processo de *eliminação de Gauss*, enquanto a matriz $L$ é composta pelos *fatores de multiplicação* usados para zerar cada elemento no processo.

Note que o fato da matriz $L$ ser composta pelas operações inversas utilizadas no escalonamento garante que a operação é reversível, portanto os elementos da matriz $B$ não se alteram de posição mesmo que as linhas de $A$ sejam alteradas em $U$ pelo processo de escalonamento.

** Métodos iterativos
Os métodos iterativos consistem na aplicação de sucessivas operações em um dado valor inicial para se obter uma *solução aproximada* para um sistema linear.

Em alguns casos esse tipo de método não é capaz de encontrar uma solução, isso ocorre quando o método não converge para o valor da solução. Sendo assim, a aplicação desse tipo de método deve sempre estar acompanhada de uma *análise de convergência* para o dado problema. Apesar disso, em algumas situações os métodos iterativos são preferíveis aos métodos diretos, principalmente quando o sistema linear é de *grande porte*, sistemas para os quais uma solução por um método direto representa um grande custo computacional.

*** Método do ponto fixo
O método do ponto fixo se baseia na ideia de que há pontos (chamados de pontos fixos) de funções que, quando aplicados à função resultam neles mesmos.

A partir de uma equação da forma $g(x) = 0$, é possível reescrevê-la na forma $x = f(x)$. Dessa forma, é possível criar uma *sequência recursiva* na forma:

$$x_{n} = f(x_{n-1})$$

Sendo que cada $x_n$ é uma nova *aproximação do ponto fixo*. Dessa forma, se a função $f$ é uma função para a qual o método do ponto fixo converge, então a sequência $(x_n)$ *converge para o ponto fixo* da função, ou seja, o ponto $x$ tal que $f(x) = 0$.

Note que é necessário assumir um valor inicial para $x_0$, esse valor é uma aproximação inicial, que será refinado através das sucessivas iterações de aplicação do método.

Veja que é necessário verificar se a função $f$ é uma função para a qual o método converge, isso pode ser verificado através do seguinte critério:

Seja $x=f(x)$ uma equação escrita na forma de ponto fixo, com $f(x)$ e sua derivada contínua no intervalo $[a, b]$, o qual contém uma ou mais raízes desta equação. Se $|f'(x)| < 1$ para todo $x \in (a,b)$, então a sequência $x_n = f(x_{n-1})$ converge para uma raiz $\epsilon \in (a,b)$ da equação original.

Definido o método do ponto fixo, vamos agora ver como é possível aplicá-lo à resolução de sistemas lineares. Consideramos um sistema de equações lineares de solução única, com $n$ equações e variáveis:

\begin{cases}
    a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1 \\
    a_{21}x_1 + a_{22}x_2 +\cdots + a_{2n}x_n = b_2 \\
    \quad \vdots \quad + \quad \vdots \quad + \ddots + \quad \vdots \quad = \quad \vdots \\
    a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n = b_n \\
\end{cases}

Na forma matricial:

\begin{gather}

AX = B \text{, onde}
A =
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1k} \\
a_{21} & a_{22} & \cdots & a_{2k} \\
\vdots & \vdots &\ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nk}
\end{bmatrix}
\quad
X =
\begin{bmatrix}
x_1\\
x_2\\
\vdots\\
x_n\\
\end{bmatrix}
B =
\begin{bmatrix}
b_1\\
b_2\\
\vdots\\
b_n\\
\end{bmatrix}

\end{gather}

A convergência dos métodos iterativos depende do sistema linear considerado. Existem algumas *condições de convergência* que podem ser expressas em termos de normas vetoriais ou matriciais. Para os dois métodos apresentados a seguir, uma *condição suficiente para a convergência* é que a matriz $A$ seja *diagonalmente dominante*, isto é, se para cada linha da matriz o valor absoluto do elemento diagonal da linha é maior que a soma dos valores absolutos dos outros elementos da linha.

**** Método de Jacobi
O método de Jacobi consiste em reescrever a equação matricial $AX = B$ em uma forma de ponto fixo $X = F(X)$. Para isso pode-se isolar os termos diagonais da matriz no sistema, chegando a um método iterativo cuja equação de recorrência é:

$$X^{(n)} = P + QX^{x-1}$$

onde $P = D^{-1}B$, $Q= I - D^{-1}A$ e $D$ é a matriz diagonal:

\begin{gather}

D=
\begin{bmatrix}
a_{11} & 0 & \cdots & 0 \\
0 & a_{22} & \cdots & 0 \\
\vdots & \vdots &\ddots & \vdots \\
0 & 0 & \cdots & a_{nk}
\end{bmatrix}

\end{gather}

Dessa forma, basta calcular os termos $P$ e $Q$ uma única vez e depois disso aplicar os valores de $X^{(n-1)}$ na equação de recorrência para obter os valores de $X^{(n)}$. Veja que é necessário iniciar com um valor para $X^{(0)}$.

**** Método de Gauss-Seidel
O método de Gauss-Seidel é muito semelhante ao método de Jacobi. A diferença entre os métodos consiste no fato de que, enquanto no método de Jacobi todos os elementos de $X^{(n)}$ são computados a partir de $X^{(n-1)}$, no método de Gauss-Seidel os elementos de $X^{(n)}$ já computados são usados para aproximar os elementos restantes de $X^{(n)}$.

Decompondo a matriz dos coeficientes do sistema linear $A$ numa soma da forma $A = D + L + U$ tal que:

\begin{gather}

D=
\begin{bmatrix}
a_{11} & 0 & \cdots & 0 \\
0 & a_{22} & \cdots & 0 \\
\vdots & \vdots &\ddots & \vdots \\
0 & 0 & \cdots & a_{nk}
\end{bmatrix}
\quad
L=
\begin{bmatrix}
0 & 0 & \cdots & 0 \\
a_{21} & 0 & \cdots & 0 \\
\vdots & \vdots &\ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & 0
\end{bmatrix}
\quad
U=
\begin{bmatrix}
0 & a_{12} & \cdots & a_{1n} \\
0 & 0 & \cdots & a_{2n} \\
\vdots & \vdots &\ddots & \vdots \\
0 & 0 & \cdots & 0
\end{bmatrix}

\end{gather}

Com essa decomposição, o sistema original $AX = B$ pode ser rescrito na forma $(D+L)X = B - UX$, que pode ser convertida na seguinte equação de recorrência:

$$X^{(n)} = P - QX^{(n-1)}$$

onde $P=(D+L)^{-1}B$ e $Q=(D+L)^{-1}U$.

Dessa forma, os termos $P$ e $Q$ também podem ser calculados uma única vez e após isso pode-se aplicar os valores de $X^{(n-1)}$ na equação de recorrência para obter os valores de $X^{(n)}$.

* Resolução de equações não lineares
A solução de equações não-lineares na forma $f(x) = 0$ nem sempre é simples. Quando tratamos de funções mais complexas, como polinômios de graus elevados, é praticamente impossível obter uma solução usando métodos convencionais. Por conta disso foram desenvolvidos métodos numéricos para resolver equações nessa forma.

** Método da Bissecção
O método da bissecção supõe apenas que a função $f$ seja contínua no intervalo $[a,b]$, sendo um dos métodos mais simples para determinar se existem raízes da função em um intervalo.

O seguinte resultado proporciona a base para esse método:

Seja $f: [a, b] \to \mathbb{R}$ uma função *contínua* tal que $f(a)f(b) < 0$, ou seja, $f$ *muda de sinal* no intervalo $[a,b]$. Então existe pelo menos um ponto $\bar{x} \in [a,b]$ tal que $f(\bar{x}) = 0$. Além disso, se $f'$ não muda de sinal em $(a,b)$, então $\bar{x}$ é a única raiz de $f$ nesse intervalo.

Com base no resultado acima, desenvolveu-se o seguinte método para encontrar uma aproximação para a raiz de $f$:

Consideramos o intervalo $I_0 = [a_0, b_0] = [a, b]$, onde $f$ muda de sinal, como intervalo inicial. Seja $x_M$ o *ponto médio* de $I_0$, ou seja, $x_M = \frac{(a_0 + b_0)}{2}$. Como $f(a)f(b) < 0$, sabe-se que $f(x_M)$ terá o mesmo sinal que $f(a)$ ou $f(b)$. Assim, escolhemos o *novo intervalo* $I_1 = [a_1, b_1]$ como sendo:

- Se $f(x_M) = 0$ (ou menor que uma precisão dada), temos que $x_M$ é *raiz* de $f$;
- $I_1 = [a, x_M]$ se $f(a)f(x_M) < 0$;
- $I_1 = [x_M, b]$ se $f(x_M)f(b) < 0$;

Dessa forma, a cada iteração obtemos um novo intervalo $I_i$ que *contém a raiz com metade do comprimento do intervalo anterior*. Note que, embora simples, o método da bissecção tem uma velocidade de convergência lenta. Após $n$ iterações, a aproximação obtida para a raiz da equação tem um *erro absoluto* dado por

$$|x-x^*| \leq \frac{b-a}{2^{n+1}}$$

** Método do Ponto Fixo
Um *ponto fixo* de uma dada função $\varphi$ é o número $p$ que quando aplicado na função resulta nele mesmo, ou seja, $\varphi(p) = p$.

A ideia do método do ponto fixo é *associar* o problema de se determinar os *pontos fixos* ao problema de se encontrar as *raízes* de uma função. Para encontrar as raízes de $f$, podemos escrever $f(x) = x - \varphi(x)$ e encontrar um ponto fixo de $\varphi$. Nesse caso, o ponto fixo de $\varphi$ será também a raiz de $f$.

Note que é necessário definir algum *método para a escolha da função* $\varphi$. Em geral, os seguintes resultados são levados em conta:

- Se $\varphi$ é contínua no intervalo $I = [a,b]$ e $\varphi(x) \in I$ para todo $x \in I$, então $\varphi$ *terá um ponto fixo* em $I$;
- Se $\varphi '$ existir em $(a,b)$ e existir uma constante $\alpha$ tal que  $|\varphi '(x)| \leq \alpha < 1$ para todo $x \in (a,b)$, então o ponto fixo será *único*.

Em geral, basta escolher uma função $\varphi$ que *satisfaça os dois resultados* acima.

Note que resolução da equação $x = \varphi(x)$ é um processo iterativo, definindo uma sequência $(p_n)$ tal que $p_{n+1} = \varphi(p_n)$. Dessa forma, se $p_n$ *convergir* para um valor $p$, então essa será a solução para $x = \varphi(x)$.

* Aproximação de funções
* Integração numérica de funções
* Resolução de sistemas de equações diferenciais
